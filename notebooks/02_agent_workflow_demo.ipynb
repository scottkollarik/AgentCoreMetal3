{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d57076",
   "metadata": {},
   "source": [
    "# 02. Agent Workflow Demo\n",
    "\n",
    "This notebook demonstrates the complete workflow of AgentCore, from planning to execution.\n",
    "\n",
    "## Workflow Steps\n",
    "1. Task Planning\n",
    "2. Plan Validation\n",
    "3. Execution\n",
    "4. Result Processing\n",
    "5. Memory Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e99451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.append('..')\n",
    "\n",
    "from agents.planning_agent import PlanningAgent\n",
    "from agents.execution_agent import ExecutionAgent\n",
    "from utils.logging_panel import BaseLoggingPanel\n",
    "from llm.interface import OllamaLLMProvider\n",
    "from utils.memory_interface import VectorMemoryProvider, ErrorMemoryProvider\n",
    "from utils.path_utils import ensure_directory_exists, get_project_root\n",
    "\n",
    "# Get project root and ensure data directories exist\n",
    "project_root = get_project_root()\n",
    "vector_store_dir = ensure_directory_exists(\"data/vector_store\", project_root)\n",
    "error_store_dir = ensure_directory_exists(\"data/error_store\", project_root)\n",
    "\n",
    "# Set up shared providers\n",
    "llm_provider = OllamaLLMProvider(\n",
    "    model=\"mistral\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500,  # Reduced from 1000 to a more reasonable value\n",
    "    context_window=8192\n",
    ")\n",
    "\n",
    "# Set up shared logger\n",
    "logger = BaseLoggingPanel()\n",
    "\n",
    "# Set up memory providers\n",
    "vector_memory = VectorMemoryProvider(\n",
    "    config={\n",
    "        \"type\": \"vector\",\n",
    "        \"embedding_model\": \"mxbai-embed-large\",\n",
    "        \"max_history\": 10,\n",
    "        \"index_name\": \"pasta_recipes\",\n",
    "        \"base_url\": \"http://localhost:11434\",  # Ollama's default URL\n",
    "        \"persist_directory\": vector_store_dir\n",
    "    }\n",
    ")\n",
    "\n",
    "error_memory = ErrorMemoryProvider(\n",
    "    config={\n",
    "        \"use_vector_store\": True,\n",
    "        \"embedding_model\": \"mxbai-embed-large\",\n",
    "        \"max_history\": 10,\n",
    "        \"index_name\": \"error_patterns\",\n",
    "        \"base_url\": \"http://localhost:11434\",  # Ollama's default URL\n",
    "        \"persist_directory\": error_store_dir\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the task\n",
    "task = \"Find a suitable homemade pasta recipe that supports hand-twisted pasta shapes.  Provide photos of the pasta shapes. Also create a chart of the interest in time from 1984 onwards.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create planning agent with config\n",
    "planner = PlanningAgent(\n",
    "    config={\n",
    "        \"model_type\": \"ollama\",\n",
    "        \"model_name\": \"mistral\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 500,\n",
    "        \"debug\": True  # Enable debug mode for streaming output\n",
    "    },\n",
    "    llm_provider=llm_provider,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# Generate plan with streaming output\n",
    "print(\"\\nGenerating plan...\")\n",
    "plan = await planner.plan(task)\n",
    "\n",
    "# Display the steps\n",
    "print(\"\\nGenerated Plan:\")\n",
    "for i, step in enumerate(plan, 1):\n",
    "    print(f\"\\nStep {i}:\")\n",
    "    print(f\"Description: {step['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e237304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create execution agent with separate debug panel\n",
    "execution_logger = BaseLoggingPanel(title=\"Execution Agent Debug Logs\")\n",
    "executor = ExecutionAgent(\n",
    "    config={\n",
    "        \"model_type\": \"ollama\",\n",
    "        \"model_name\": \"mistral\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 500,\n",
    "        \"debug\": True\n",
    "    },\n",
    "    llm_provider=llm_provider,\n",
    "    logger=execution_logger  # Use the separate debug panel\n",
    ")\n",
    "\n",
    "# Execute each step\n",
    "results = []\n",
    "for step in plan:\n",
    "    print(f\"\\nExecuting Step {step['step']}:\")\n",
    "    print(f\"Description: {step['description']}\")\n",
    "    \n",
    "    # Execute the step\n",
    "    result = await executor.execute(step)\n",
    "    results.append(result)\n",
    "    \n",
    "    # Display the result\n",
    "    print(f\"Result: {'Success' if result['success'] else 'Failed'}\")\n",
    "    print(f\"Output: {result['output']}\")\n",
    "    if not result['success']:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
